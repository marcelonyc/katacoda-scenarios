{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MLRUN function locally, as a Kubernetes Job, and in a Workflow\n",
    "  --------------------------------------------------------------------\n",
    "\n",
    "#### **notebook how-to's**\n",
    "* Write and test code in a notebook.\n",
    "* Convert it to a containerized image.\n",
    "* Run it on a Kubernetes cluster with shared file or object storage.\n",
    "* Run it in an automated workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for use on Iguazio platform\n",
    "trainer.apply(mount_v3io())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>\n",
    "#### **steps**\n",
    "**[intall mlrun](#install)**<br>\n",
    "**[define a new function and its dependencies](#define-function)**<br>\n",
    "**[test the function code and pipeline locally](#test-locally)**<br>\n",
    "**[define cluster jobs and build images](#build)**<br>\n",
    "**[deploy (build) the function container](#deploy-build)**<br>\n",
    "**[run the function on the cluster](#run-on-cluster)**<br>\n",
    "**[create and run a KubeFlow Pipeline](#create-pipeline)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"install\" ></a>\n",
    "______________________________________________\n",
    "### **install mlrun**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to install mlrun package, restart the kernel after\n",
    "\n",
    "# !pip install -U mlrun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the UI external URL (will generate ui hyperlinks)\n",
    "# %env MLRUN_UI_URL=http://<mlrun-ui-url>:<port>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='define-function'></a>\n",
    "### **define a new function and its dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: ignore\n",
    "# do not remove the comment above (it is a directive to nuclio, ignore that cell during build)\n",
    "# if the nuclio-jupyter package is not installed run !pip install nuclio-jupyter and restart the kernel \n",
    "import nuclio "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `%nuclio` magic commands to set package dependencies and configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nuclio cmd -c pip install pandas\n",
    "%nuclio config spec.build.baseImage = \"mlrun/mlrun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```DataItem```s and the ```context``` within which they are logged are described in the following ```mlrun``` modules (they are included here only for type clarity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun.execution import MLClientCtx\n",
    "from mlrun.datastore import DataItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def training(\n",
    "    context: MLClientCtx,\n",
    "    p1: int = 1,\n",
    "    p2: int = 2\n",
    ") -> None:\n",
    "    \"\"\"Train a model.\n",
    "\n",
    "    :param context: The runtime context object.\n",
    "    :param p1: A model parameter.\n",
    "    :param p2: Another model parameter.\n",
    "    \"\"\"\n",
    "    # access input metadata, values, and inputs\n",
    "    print(f'Run: {context.name} (uid={context.uid})')\n",
    "    print(f'Params: p1={p1}, p2={p2}')\n",
    "    context.logger.info('started training')\n",
    "    \n",
    "    # <insert training code here>\n",
    "    \n",
    "    # log the run results (scalar values)\n",
    "    context.log_result('accuracy', p1 * 2)\n",
    "    context.log_result('loss', p1 * 3)\n",
    "    \n",
    "    # add a lable/tag to this run \n",
    "    context.set_label('category', 'tests')\n",
    "    \n",
    "    # log a simple artifact + label the artifact \n",
    "    # If you want to upload a local file to the artifact repo add src_path=<local-path>\n",
    "    context.log_artifact('model', \n",
    "                          body=b'abc is 123', \n",
    "                          local_path='model.txt', \n",
    "                          labels={'framework': 'tfkeras'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(\n",
    "    context: MLClientCtx,\n",
    "    model: DataItem\n",
    ") -> None:\n",
    "    \"\"\"Model validation.\n",
    "    \n",
    "    Dummy validation function.\n",
    "    \n",
    "    :param context: The runtime context object.\n",
    "    :param model: The extimated model object.\n",
    "    \"\"\"\n",
    "    # access input metadata, values, files, and secrets (passwords)\n",
    "    print(f'Run: {context.name} (uid={context.uid})')\n",
    "    print(f'file - {model.url}:\\n{model.get()}\\n')\n",
    "    context.logger.info('started validation')    \n",
    "    context.log_artifact('validation', \n",
    "                         body=b'<b> validated </b>', \n",
    "                         format='html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following end-code annotation tells ```nuclio``` to stop parsing the notebook from this cell. _**Please do not remove this cell**_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nuclio: end-code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='test-locally'></a>\n",
    "### **test the function code and pipeline locally**\n",
    "The functions above can be tested locally. Parameters, inputs, and outputs can be specified in the API or the `Task` object.\n",
    "\n",
    "We create a ```function``` which defines the runtime environment (type, code, image, ..) and ```run()``` a job or experiments using that function.\n",
    "\n",
    "We use the ```local``` runtime by default, later on we will use a ```job``` runtime for running containers, and can use other distributed runners like MpiJob, Spark, Dask, and Nuclio.\n",
    "\n",
    "In each run we can specify the function, inputs, parameters/hyper-parameters, etc... For more details, see the [mlrun_basics notebook](mlrun_basics.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import run_local, code_to_function, mlconf, NewTask\n",
    "from mlrun.platforms import mount_pvc\n",
    "mlconf.dbpath = mlconf.dbpath or 'http://mlrun-api:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> define the artifact location</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "out = mlconf.artifact_path or path.abspath('./data')\n",
    "# {{run.uid}} will be substituted with the run id, so output will be written to different directoried per run\n",
    "artifact_path = path.join(out, '{{run.uid}}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _running and linking multiple tasks_\n",
    "In this example we run two functions, ```training``` and ```validation``` and we pass the result from one to the other.\n",
    "We will see in the ```job``` example that linking works even when the tasks are run in a workflow on different processes or containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```run_local()``` will run our task on a local function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the training function. Functions can have multiple handlers/methods, here we call the ```training``` handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run = run_local(NewTask(handler=training, params={'p1': 5}, artifact_path=out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function runs it generates the result widget, you can click the `model` artifact to see its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_run.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output from the first training function is passed to the validation function, let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = train_run.outputs['model']\n",
    "\n",
    "validation_run = run_local(NewTask(handler=validation, inputs={'model': model_path}, artifact_path=out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "### **define cluster jobs and build images**\n",
    "\n",
    "In order to use our function in a cluster we need to package our code and dependencies.\n",
    "\n",
    "The ```code_to_function``` call will automatically generate a ```function``` object from the current notebook (or a specified file) with its list of dependencies and runtime configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an ML function from the notebook, attache it to iguazio data fabric (v3io)\n",
    "trainer = code_to_function(name='my-trainer', kind='job')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions need shared storage (file or object) media to pass and store artifacts.\n",
    "\n",
    "You can add _**Kubernetes**_ resources like volumes, environment variables, secrets, cpu/mem/gpu, etc. to a function.\n",
    "\n",
    "```mlrun``` uses _**KubeFlow**_ modifiers (apply) to configure resources, you can build your own or use predefined ones e.g. for [AWS resources](https://github.com/kubeflow/pipelines/blob/master/sdk/python/kfp/aws.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _**Option 1: Using file volumes for artifacts**_\n",
    "If your are using [Iguazio data science platform](https://www.iguazio.com/) use the `mount_v3io()` auto-mount modifier.<br>\n",
    "if you use other k8s PVC volumes you can use the `mlrun.platforms.mount_pvc(..)` modifier with the requiered params.\n",
    "\n",
    "Applying ```mount_v3io()``` will attach the function to Iguazio's real-time data fabric (mounted by default to _**home**_ of the current user).\n",
    "\n",
    "**Note**: if the notebook is not on the managed platform (running remotely) you need to create and use a v3io secret, run:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl create -n <namespace> secret generic my-v3io --from-literal=accessKey=<your access key> --from-literal=username=<your user name> --type v3io/fuse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and use: `trainer.apply(mount_v3io(user='admin', secret='my-v3io'))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for our current ```training``` function, when using Iguazio data science platform run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment for use with shared PVC volumes, e.g. using the NFS Share in the local k8s install\n",
    "# from mlrun.platforms import mount_pvc\n",
    "# trainer.apply(mount_pvc('nfsvol', 'nfsvol', '/home/jovyan/data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### _**Option 2: Using AWS S3 for artifacts**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In AWS you can use S3 and need to have a `secret` with AWS credentials. An AWS secret can be created with the following command line:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl create -n <namespace> secret generic my-aws --from-literal=AWS_ACCESS_KEY_ID=<access key> --from-literal=AWS_SECRET_ACCESS_KEY=<secret key>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the secret:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kfp.aws import use_aws_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.apply(use_aws_secret(secret_name='my-aws'))\n",
    "# out = 's3://<your-bucket-name>/jobs/{{run.uid}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"deploy-build\"></a>\n",
    "### **deploy (build) the function container**\n",
    "\n",
    "The `deploy()` command will build a custom container image (create a cluster build job) from the outlined function dependencies.\n",
    "\n",
    "If a pre-built container image already exists, pass the `image` name instead. _**Note that the code and params can be updated per run without building a new image**_.\n",
    "\n",
    "The image is stored in a container repository, and by default it uses the repository configured on the MLRun API service, you can specify your own docker registry by first creating a secret, and adding that secret name to the build configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`kubectl create -n <namespace> secret docker-registry my-docker --docker-server=https://index.docker.io/v1/ --docker-username=<your-user> --docker-password=<your-password> --docker-email=<your-email>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and run this: `trainer.build_config(image='target/image:tag', secret='my_docker')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"run-on-cluster\"></a>\n",
    "### **run the function on the cluster**\n",
    "\n",
    "\n",
    "In case we made changes to the code, ```with_code``` will inject the latest code into the function (it doesn't require a new build)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.with_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.apply(mount_pvc(pvc_name='pvclocal',volume_name='pvclocal',volume_mount_path=\"/home/jovyan/mlrun\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base task (common to both steps), and set the output path and experiment label\n",
    "base_task = NewTask(artifact_path=out).set_label('stage', 'dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our training task, with hyper params, and select the one with max accuracy\n",
    "train_task = NewTask(name='my-training', handler='training', params={'p1': 9}, base=base_task)\n",
    "train_run = trainer.run(train_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# running validation, use the model result from the previos step \n",
    "model_path = train_run.outputs['model']\n",
    "trainer.run(base_task, handler='validation', inputs={'model': model_path}, watch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create-pipeline\"></a>\n",
    "### **create and run a KubeFlow pipeline**\n",
    "\n",
    "KubeFlow pipelines are used for workflow automation--we compose a graph of functions and specify parameters, inputs and outputs.\n",
    "\n",
    "As ilustrated below, we can chain the outputs and inputs of the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from mlrun import run_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name = 'job test',\n",
    "    description = 'demonstrating mlrun usage'\n",
    ")\n",
    "def job_pipeline(\n",
    "   p1: int = 9\n",
    ") -> None:\n",
    "    \"\"\"Define our pipeline.\n",
    "    \n",
    "    :param p1: A model parameter.\n",
    "    \"\"\"\n",
    "\n",
    "    train = trainer.as_step(handler='training',\n",
    "                            params={'p1': p1},\n",
    "                            outputs=['model'])\n",
    "    \n",
    "    validate = trainer.as_step(handler='validation',\n",
    "                               inputs={'model': train.outputs['model']},\n",
    "                               outputs=['validation'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job pipeline can compiled to a yaml file that can be used for debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(job_pipeline, 'jobpipe.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline results are stored at the `artifact_path` location:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, by adding ```/{{workflow.uid}}``` to the path ```mlrun``` will generate a unique folder per workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_path = 'v3io:///users/admin/kfp/{{workflow.uid}}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = {'p1': 8}\n",
    "run_id = run_pipeline(job_pipeline, arguments, experiment='my-job', artifact_path=artifact_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
